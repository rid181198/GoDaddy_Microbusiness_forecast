{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ef2d60",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-12T12:56:25.889957Z",
     "iopub.status.busy": "2023-01-12T12:56:25.889073Z",
     "iopub.status.idle": "2023-01-12T12:56:34.473823Z",
     "shell.execute_reply": "2023-01-12T12:56:34.472073Z"
    },
    "papermill": {
     "duration": 8.595015,
     "end_time": "2023-01-12T12:56:34.477328",
     "exception": false,
     "start_time": "2023-01-12T12:56:25.882313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from joblib import Parallel, delayed\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6702d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T12:56:34.487964Z",
     "iopub.status.busy": "2023-01-12T12:56:34.487104Z",
     "iopub.status.idle": "2023-01-12T12:56:35.009279Z",
     "shell.execute_reply": "2023-01-12T12:56:35.007869Z"
    },
    "papermill": {
     "duration": 0.531063,
     "end_time": "2023-01-12T12:56:35.012666",
     "exception": false,
     "start_time": "2023-01-12T12:56:34.481603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/godaddy-microbusiness-density-forecasting/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/godaddy-microbusiness-density-forecasting/test.csv')\n",
    "census_data = pd.read_csv('/kaggle/input/godaddy-microbusiness-density-forecasting/census_starter.csv')\n",
    "\n",
    "min_date = pd.to_datetime(min(train_data['first_day_of_month']))\n",
    "max_date = pd.to_datetime(max(train_data['first_day_of_month']))\n",
    "margin =  ( max_date.to_period('M')  ) \\\n",
    "                                   - ( min_date.to_period('M')  ) \n",
    "\n",
    "dict_val = {}\n",
    "for leng, val in enumerate(train_data.cfips.unique()):\n",
    "    dict_val[val] = leng\n",
    "    \n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#data for broadband access\n",
    "census_data.iloc[:,0:5]=scaler.fit_transform(census_data.iloc[:,0:5].to_numpy())\n",
    "\n",
    "#population percent of 4yr degree\n",
    "census_data.iloc[:,6:11]=scaler.fit_transform(census_data.iloc[:,6:11].to_numpy())\n",
    "\n",
    "#percent of born outside of the US\n",
    "census_data.iloc[:,11:16]=scaler.fit_transform(census_data.iloc[:,11:16].to_numpy())\n",
    "\n",
    "#percent of IT workers\n",
    "census_data.iloc[:,16:21]=scaler.fit_transform(census_data.iloc[:,16:21].to_numpy())\n",
    "\n",
    "#income level\n",
    "census_data.iloc[:,21:26]=scaler.fit_transform(census_data.iloc[:,21:26].to_numpy())\n",
    "\n",
    "def census_to_train(row,idNum,dataset):\n",
    "    row=row\n",
    "    idNum = idNum\n",
    "    year=pd.to_datetime(train_data['first_day_of_month'][row]).year - 2\n",
    "    temp_cen_data = pd.DataFrame(census_data[census_data['cfips'] == idNum])\n",
    "    \n",
    "    dataset.loc[row,\"pct_bb\"] = float(temp_cen_data[\"pct_bb_\" + str(year)])\n",
    "    dataset.loc[row,\"pct_college\"] = float(temp_cen_data[\"pct_college_\"+ str(year)])\n",
    "    dataset.loc[row,\"pct_foreign_born\"] = float(temp_cen_data[\"pct_foreign_born_\"+ str(year)])\n",
    "    dataset.loc[row,\"pct_it_workers\"] = float(temp_cen_data[\"pct_it_workers_\"+ str(year)])\n",
    "    dataset.loc[row,\"median_hh_inc\"] = float(temp_cen_data[\"median_hh_inc_\"+ str(year)])\n",
    "\n",
    "\n",
    "def init_to_feature(dataset,LSTM=False,Test=False):\n",
    "    #dataset['numMons'] = \" \"\n",
    "    dates = np.array(dataset['first_day_of_month'])\n",
    "\n",
    "    #numMons = ( (pd.to_datetime(dates).to_period('M')   ) \\\n",
    "    #                               - ( min_date.to_period('M')  ) )\n",
    "    #for i in range(len(dataset)):\n",
    "    #    dataset.loc[i,'numMons'] = numMons[i].n +1\n",
    "    dataset['numMons'] = \" \"\n",
    "    dataset['numMons'] = np.repeat(np.arange(1,margin.n+2,1), len(dict_val))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset[\"countyID\"] = \" \"\n",
    "    for leng, val in enumerate(dataset['cfips']):\n",
    "        dataset.loc[leng,\"countyID\"] = dict_val[dataset.loc[leng,'cfips']]\n",
    "       \n",
    "    dataset = dataset.sort_values(['countyID', 'numMons'],\\\n",
    "              ascending = [True, True])\n",
    "\n",
    "    dataset[\"pct_bb\"] = \" \"\n",
    "    dataset[\"pct_college\"] = \" \"\n",
    "    dataset[\"pct_foreign_born\"] = \" \"\n",
    "    dataset[\"pct_it_workers\"] = \" \"\n",
    "    dataset[\"median_hh_inc\"] = \" \"\n",
    "\n",
    "    \n",
    "    \n",
    "    for row, idNum in enumerate(tqdm(dataset['cfips'])):\n",
    "         census_to_train(row,idNum,dataset)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    if LSTM==True and Test==False:\n",
    "        new_set=[]\n",
    "        for i in np.arange(0, len(dataset),39):\n",
    "            new_set.append(dataset[i:i+30])\n",
    "        dataset = pd.concat(new_set)\n",
    "        \n",
    "    if LSTM==True and Test==True:\n",
    "        new_set=[]\n",
    "        for i in np.arange(0, len(dataset),39):\n",
    "            new_set.append(dataset[i+25:i+39])\n",
    "        dataset = pd.concat(new_set)\n",
    "    \n",
    "    \n",
    "    features = np.array(dataset[['countyID','numMons','pct_bb','pct_college',\\\n",
    "                               'pct_foreign_born','pct_it_workers','median_hh_inc','microbusiness_density']])\n",
    "\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462f716d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T12:56:35.022800Z",
     "iopub.status.busy": "2023-01-12T12:56:35.022223Z",
     "iopub.status.idle": "2023-01-12T15:07:01.653026Z",
     "shell.execute_reply": "2023-01-12T15:07:01.651666Z"
    },
    "papermill": {
     "duration": 7826.639674,
     "end_time": "2023-01-12T15:07:01.656318",
     "exception": false,
     "start_time": "2023-01-12T12:56:35.016644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122265/122265 [58:11<00:00, 35.02it/s]\n",
      "100%|██████████| 122265/122265 [1:00:16<00:00, 33.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#test_features = init_to_feature(test_data,LSTM=False)\n",
    "\n",
    "train_features = init_to_feature(train_data[:],LSTM=True,Test=False)\n",
    "test_features = init_to_feature(train_data[:],LSTM=True,Test=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7413c78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:07:09.725783Z",
     "iopub.status.busy": "2023-01-12T15:07:09.725334Z",
     "iopub.status.idle": "2023-01-12T15:07:09.733005Z",
     "shell.execute_reply": "2023-01-12T15:07:09.731855Z"
    },
    "papermill": {
     "duration": 4.002599,
     "end_time": "2023-01-12T15:07:09.735200",
     "exception": false,
     "start_time": "2023-01-12T15:07:05.732601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_target = np.array(train_features[:,7])\n",
    "test_target = np.array(test_features[:,7])\n",
    "\n",
    "train_features = train_features[:,:7]\n",
    "test_features = test_features[:,:7]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ec001d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:07:17.812776Z",
     "iopub.status.busy": "2023-01-12T15:07:17.812323Z",
     "iopub.status.idle": "2023-01-12T15:07:17.819595Z",
     "shell.execute_reply": "2023-01-12T15:07:17.818239Z"
    },
    "papermill": {
     "duration": 4.09209,
     "end_time": "2023-01-12T15:07:17.822147",
     "exception": false,
     "start_time": "2023-01-12T15:07:13.730057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def history_data(dataset1,dataset2, lookback=1):\n",
    "    data_look=[]\n",
    "    data_tar=[]\n",
    "    for i in range(len(dataset1)-lookback):\n",
    "        val1 = dataset1[i:(i+lookback),:] \n",
    "        val2 = dataset2[i+lookback] \n",
    "        data_look.append(val1)\n",
    "        data_tar.append(val2)\n",
    "    return np.array(data_look), np.array(data_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a01cf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:07:25.671007Z",
     "iopub.status.busy": "2023-01-12T15:07:25.669418Z",
     "iopub.status.idle": "2023-01-12T15:07:25.678734Z",
     "shell.execute_reply": "2023-01-12T15:07:25.677550Z"
    },
    "papermill": {
     "duration": 4.000254,
     "end_time": "2023-01-12T15:07:25.681372",
     "exception": false,
     "start_time": "2023-01-12T15:07:21.681118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lookback_data(ids,featureset,targetset):\n",
    "   \n",
    "    dataset1 = featureset[featureset[:,0]==ids]\n",
    "    dataset2 = targetset[featureset[:,0]==ids]\n",
    "    rnnset, tar_rnn = history_data(dataset1,dataset2,5)\n",
    "\n",
    "    rnnset = np.delete(rnnset,0, axis=2)\n",
    "    rnnset = np.reshape(rnnset, (rnnset.shape[0],rnnset.shape[1],rnnset.shape[2]   ))\n",
    "    rnnset = rnnset.astype(float)\n",
    "    \n",
    "    return rnnset, tar_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552f2a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:07:33.365898Z",
     "iopub.status.busy": "2023-01-12T15:07:33.365454Z",
     "iopub.status.idle": "2023-01-12T15:07:34.369100Z",
     "shell.execute_reply": "2023-01-12T15:07:34.367530Z"
    },
    "papermill": {
     "duration": 4.853367,
     "end_time": "2023-01-12T15:07:34.372061",
     "exception": false,
     "start_time": "2023-01-12T15:07:29.518694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 15:07:33.423870: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions=list(np.zeros(3134))\n",
    "trues=list(np.zeros(3134))\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=200,return_sequences=True, input_shape=(5,6)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=100,return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "def lstm_model(ids):\n",
    "    \n",
    "    train_rnn, traintar_rnn = lookback_data(ids,train_features,train_target )\n",
    "    test_rnn, testtar_rnn = lookback_data(ids,test_features,test_target )\n",
    "    \n",
    "    model.fit(train_rnn,traintar_rnn,epochs=50,batch_size=8,verbose=0)\n",
    "    \n",
    "    predictions[ids] = model.predict(test_rnn)\n",
    "    trues[ids] = testtar_rnn\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d6fafc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T15:07:42.461706Z",
     "iopub.status.busy": "2023-01-12T15:07:42.461226Z",
     "iopub.status.idle": "2023-01-12T18:41:00.820627Z",
     "shell.execute_reply": "2023-01-12T18:41:00.819461Z"
    },
    "papermill": {
     "duration": 12802.438044,
     "end_time": "2023-01-12T18:41:00.823049",
     "exception": false,
     "start_time": "2023-01-12T15:07:38.385005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3134 [00:00<?, ?it/s]2023-01-12 15:07:42.542240: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "100%|██████████| 3134/3134 [3:33:18<00:00,  4.08s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ids in tqdm(range(0,3134)):\n",
    "    lstm_model(ids)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f0a1e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T18:41:09.251305Z",
     "iopub.status.busy": "2023-01-12T18:41:09.250826Z",
     "iopub.status.idle": "2023-01-12T18:41:09.522064Z",
     "shell.execute_reply": "2023-01-12T18:41:09.520777Z"
    },
    "papermill": {
     "duration": 4.575178,
     "end_time": "2023-01-12T18:41:09.524614",
     "exception": false,
     "start_time": "2023-01-12T18:41:04.949436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAPE:  [nan]\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "summ=0\n",
    "for tr1,pred1 in zip(trues,predictions):\n",
    "    for tr2, pred2 in zip(tr1,pred1):\n",
    "        summ+= abs((-tr2+pred2)/((abs(tr2)+abs(pred2))/2) )\n",
    "        counter+=1\n",
    "print('SMAPE: ', summ/counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0cf56",
   "metadata": {
    "papermill": {
     "duration": 4.048817,
     "end_time": "2023-01-12T18:41:17.664216",
     "exception": false,
     "start_time": "2023-01-12T18:41:13.615399",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20710.059198,
   "end_time": "2023-01-12T18:41:24.886376",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-12T12:56:14.827178",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
